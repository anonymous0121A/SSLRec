optimizer:
  name: adam
  lr: 1.0e-3 # not 1e-3
  weight_decay: 0

train:
  epoch: 200
  batch_size: 512
  save_model: true
  test_step: 1 # evaluate per {test_step} epochs
  #pretrain_path: ./checkpoint/xxxx.pth
  reproducible: true
  seed: 2023

test:
  metrics: [recall, ndcg] # choose in {ndcg, recall, precision, mrr}
  k: [5, 10] # top-k
  batch_size: 512 # How many users per batch during validation

data:
  type: sequential # choose in {general_cf, multi_behavior, sequential, social}
  name: ml-20m

model:
  name: bert4rec # case-insensitive
  dropout_rate: 0.1
  n_layers: 2
  embedding_size: 64
  mask_prob: 0.2
  n_heads: 2
  max_seq_len: 50

tune:
  enable: false # Whether to enable grid search to search for optimal hyperparameters
  hyperparameters: [] # The name of the hyperparameter
  layer_num: [] # Use a list to store the search range
  reg_weight: []
