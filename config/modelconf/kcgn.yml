optimizer:
  name: adam
  lr: 5.0e-6 # 5e-6 too big, 1e-6 too small
  weight_decay: 0.98

train:
  epoch: 180
  batch_size: 512
  save_model: true
  loss: pairwise # bpr
  test_step: 5 # evaluate per {test_step} epochs
  #pretrain_path: ./checkpoint/xxxx.pth
  reproducible: true
  seed: 2023

test:
  metrics: [recall, ndcg] # choose in {ndcg, recall, precision, mrr}
  k: [10, 20, 40] # top-k
  batch_size: 512 # How many users per batch during validation

data:
  type: social # choose in {general_cf, multi_behavior, sequential, social}
  name: yelp
  clear: false

model:
  name: kcgn # case-insensitive
  layer_num: 2
  reg_weight: 1.0e-1
  embedding_size: 64
  slope: 0.4
  fuse: mean
  dgi_graph_act: sigmoid
  lam: [0.1,0.01]
  subnode: 10
  time_step: 360

tune:
  enable: false # Whether to enable grid search to search for optimal hyperparameters
  hyperparameters: [layer_num, reg_weight] # The name of the hyperparameter
  layer_num: [1, 2, 3] # Use a list to store the search range
  reg_weight: [1.0e-1, 1.0e-2, 1.0e-3]
